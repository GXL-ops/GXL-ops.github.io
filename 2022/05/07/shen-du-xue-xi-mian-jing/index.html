<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习面经 | 曼曼吖の博客</title><meta name="keywords" content="HTML,CSS,JavaScript,JQuery,React,Vue.js等"><meta name="author" content="Xinle Guo,xinleguo@outlook.com"><meta name="copyright" content="Xinle Guo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="深度学习面经1. BatchNormalization的作用为何：  神经网络在训练的时候随着网络层数的加深，激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近，从而导致在反向传播时低层的神经网络的梯度消失。 而BatchNormalization的作用是通过规范化的手段，将越来越偏的分布拉回到标准化的分布，使得激活函数的输入值落在激活函数对输入比较敏感的区域，从而使梯度变大，加快学习收">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习面经">
<meta property="og:url" content="https://xiaomanzhan.club/2022/05/07/shen-du-xue-xi-mian-jing/index.html">
<meta property="og:site_name" content="曼曼吖の博客">
<meta property="og:description" content="深度学习面经1. BatchNormalization的作用为何：  神经网络在训练的时候随着网络层数的加深，激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近，从而导致在反向传播时低层的神经网络的梯度消失。 而BatchNormalization的作用是通过规范化的手段，将越来越偏的分布拉回到标准化的分布，使得激活函数的输入值落在激活函数对输入比较敏感的区域，从而使梯度变大，加快学习收">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xiaomanzhan.club/img/acg.gy_42.png">
<meta property="article:published_time" content="2022-05-07T12:30:23.000Z">
<meta property="article:modified_time" content="2022-05-08T15:06:59.032Z">
<meta property="article:author" content="Xinle Guo">
<meta property="article:tag" content="HTML">
<meta property="article:tag" content="CSS">
<meta property="article:tag" content="JavaScript">
<meta property="article:tag" content="JQuery">
<meta property="article:tag" content="React">
<meta property="article:tag" content="Vue.js等">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xiaomanzhan.club/img/acg.gy_42.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xiaomanzhan.club/2022/05/07/shen-du-xue-xi-mian-jing/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习面经',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-08 23:06:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="/css/runtime.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="曼曼吖の博客" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/loading.gif" data-original="/img/avatar1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">101</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">76</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-image"></i><span> 时光长廊</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/acg.gy_42.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">曼曼吖の博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-image"></i><span> 时光长廊</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习面经</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-05-07T12:30:23.000Z" title="发表于 2022-05-07 20:30:23">2022-05-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-08T15:06:59.032Z" title="更新于 2022-05-08 23:06:59">2022-05-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习面经"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1><span id="深度学习面经">深度学习面经</span></h1><h2><span id="1-batchnormalization的作用">1. BatchNormalization的作用</span></h2><p>为何：</p>
<ol>
<li>神经网络在训练的时候随着网络层数的加深，激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近，从而导致在反向传播时低层的神经网络的梯度消失。</li>
<li>而BatchNormalization的作用是通过规范化的手段，将越来越偏的分布拉回到标准化的分布，使得激活函数的输入值落在激活函数对输入比较敏感的区域，从而使梯度变大，加快学习收敛速度，避免梯度消失的问题。</li>
</ol>
<p>优势：</p>
<ul>
<li>批量归一化会固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li>
<li>能够加速收敛速度，但是一般不改变模型的精度。</li>
</ul>
<h3><span id="11-bn层的作用为什么要在后面加伽马和贝塔不加可以吗">1.1 BN层的作用，为什么要在后面加伽马和贝塔，不加可以吗</span></h3><p>BN层的作用是把一个batch内的所有数据，从不规范的分布拉到正态分布。这样做的好处是使得数据能够分布在激活函数的敏感区域，敏感区域即为梯度较大的区域，因此在反向传播的时候能够较快反馈误差传播。</p>
<p>批量归一化没有起到优化作用，可以通过这两个参数进行抵消。</p>
<h2><span id="2-梯度消失">2. 梯度消失</span></h2><p><strong>说法一：</strong>本质是由于链式法则的乘法特性导致的，对于Sigmoid，导数的最大值在输入为0处，值为0.25.考虑一个激活函数都是Sigmoid的多层神经网络，则梯度向后传导时，没经过一个Sigmoid就需要乘以一个小于0.25的梯度。而每乘以一个小于0.25的梯度，则梯度的值又变得更小一些。况且在优化的过程中，每个激活层输入都在0附近的概率非常的低。也就是说随着层数的加深，梯度的衰减会非常的大，迅速接近0，这就是梯度消失的现象。</p>
<p><strong>说法二：</strong>在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫做消失的梯度问题。</p>
<p><strong>说法三：</strong>激活函数在计算 $\prod_{i = t}^{d-1}\frac{\delta h^{i+1}}{\delta h^{i}}=\prod_{i=t}^{d-1}diag(\sigma’(W^ih^{i-1}))(W^i)^T$ 的一些元素会来自于 $\prod_{i = t}^{d-1}(W^i)^T$。如果 d-t 很大（即网络层数很深），假设激活函数使用sigmoid时，每次乘以一个很小的值，结果趋近于0，呈现梯度消失。</p>
<p>解决思路：</p>
<ul>
<li><p>用ReLU代替Sigmoid；</p>
<ul>
<li>可以发现，relu函数的导数在正数部分，是等于1的，因此就可以避免梯度消失的问题。</li>
<li>但是负数部分的导数等于0，这样意味着，只要在链式法则中某一个 zj小于0，那么这个神经元的梯度就是0，不会更新。</li>
<li>解决办法：<ul>
<li><strong>【leakyReLU】</strong>：在ReLU的负数部分，增加了一定的斜率，解决了ReLU中会有死神经元的问题。</li>
<li>跟 LeakyReLU 一样是为了解决死神经元问题，但是增加的斜率不是固定的：</li>
</ul>
</li>
</ul>
</li>
<li><p>用BN层；</p>
<ul>
<li>BN层提出来的本质就是为了<strong>解决反向传播中的梯度问题</strong>。</li>
</ul>
</li>
<li><p>用残差结构解决梯度消失问题。</p>
<ul>
<li>残差结构，就是让深层网络通过走捷径，让网络不那么深层。这样梯度消失的问题就缓解了。</li>
</ul>
</li>
</ul>
<h2><span id="3-梯度爆炸">3. 梯度爆炸</span></h2><p>梯度爆炸的表现：</p>
<ol>
<li>在深度多层感知机网络中，梯度爆炸会导致网络不稳定，最好的结果是无法从训练数据中学习，</li>
<li>由于权重值为NaN而无法更新权重。</li>
<li>模型无法在训练数据上收敛（比如，损失函数值非常差，波动大）；</li>
</ol>
<p>去和确定梯度爆炸：</p>
<ol>
<li>模型在训练过程中，权重变化非常大；</li>
<li>模型在训练过程中，权重变成NaN值；</li>
<li>每层的每个节点在训练时，其误差梯度值一直是大于1.0；</li>
</ol>
<p>解决思路：</p>
<ol>
<li><strong>重新设计网络模型</strong>：<ol>
<li>将网络模型的层数变少来解决；</li>
<li>使用较小批量也有一些好处；</li>
<li>在循环神经网络中，训练时使用较小时间步长更新；</li>
</ol>
</li>
<li><strong>使用修正线性激活函数</strong>：<ol>
<li>使用修正线性激活函数（ReLU）能够减少梯度爆炸发生的概率；</li>
</ol>
</li>
<li><strong>使用长短周期记忆网络</strong>：<ol>
<li>通过使用长短期记忆单元（LSTM）或相关的门控神经结构能够减少梯度爆炸发生的概率。</li>
</ol>
</li>
<li><strong>使用梯度裁剪</strong>：<ol>
<li>在深度多层感知网络中，当有大批量数据以及LSTM是用于很长时间序列时，梯度爆炸仍然会发生。当梯度爆炸发生时，可以在网络训练时检查并限制梯度的大小，这被称作梯度裁剪。</li>
<li>梯度剪裁，若误差梯度值超过设定的阈值，则截断或设置为阈值。</li>
</ol>
</li>
<li><strong>使用权重正则化</strong>：<ol>
<li>如果梯度爆炸问题仍然发生，另外一个方法是对网络权重的大小进行校验，并对大权重的损失函数增添一项惩罚项，这也被称作权重正则化，常用的有L1（权重的绝对值和）正则化与L2（权重的绝对值平方和再开方）正则化。</li>
</ol>
</li>
</ol>
<h2><span id="4-循环神经网络为什么好">4. 循环神经网络，为什么好?</span></h2><p>循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列。使得RNN可以更加容易处理不分段的文本等</p>
<h2><span id="5-什么是group-convolution">5. 什么是Group Convolution</span></h2><p>若卷积神将网络的上一层有N个卷积核，则对应的通道数也为N。设群数目为M，在进行卷积操作的时候，将通道分成M份，每个group对应N/M个通道，然后每个group卷积完成后输出叠在一起，作为当前层的输出通道。</p>
<h2><span id="6-什么是rnn">6. 什么是RNN</span></h2><p>一个序列当前的输出与前面的输出也有关，在RNN网络结构中，隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出，网络会对之前的信息进行记忆并应用于当前的输入计算中。</p>
<h2><span id="7-训练过程中若一个模型不收敛那么是否说明这个模型无效导致模型不收敛的原因有哪些">7. 训练过程中，若一个模型不收敛，那么是否说明这个模型无效？导致模型不收敛的原因有哪些？</span></h2><ol>
<li>并不能说明这个模型无效，有可能是因为数据的问题。导致模型不收敛的原因可能有数据分类的标注不准确，数据样本量太少，样本的信息量太大导致模型不足以fit整个样本空间。</li>
<li>学习率设置的太大容易产生震荡，太小会导致不收敛。</li>
<li>可能复杂的分类任务用了简单的模型。</li>
<li>数据没有进行归一化的操作。<ol>
<li>提高模型精度；</li>
<li>加快收敛速度</li>
</ol>
</li>
</ol>
<h2><span id="8-vgg使用33卷积核的优势是什么">8. VGG使用3*3卷积核的优势是什么?</span></h2><ol>
<li>2个<code>3*3</code>的卷积核串联和<code>5*5</code>的卷积核有相同的感知野，前者拥有更少的参数。</li>
<li>多个<code>3*3</code>的卷积核比一个较大尺寸的卷积核有更多层的非线性函数，增加了非线性表达，提取的特征更加具有优势。</li>
</ol>
<h2><span id="9-图像处理中锐化和平滑的操作">9. 图像处理中锐化和平滑的操作</span></h2><ol>
<li>锐化就是通过增强高频分量来减少图像中的模糊，在增强图像边缘的同时也增加了图像的噪声。</li>
<li>平滑与锐化相反，过滤掉高频分量，减少图像的噪声是图片变得模糊。</li>
</ol>
<h2><span id="10-激活函数">10. 激活函数</span></h2><p>激活函数是用来加入非线性因素的，提高神经网络对模型的表达能力，解决线性模型所不能解决的问题。</p>
<h3><span id="101-relu比sigmoid的效果好在哪里">10.1 Relu比Sigmoid的效果好在哪里?</span></h3><ol>
<li>Sigmoid的导数只有在0的附近时有较好的激活性，而在正负饱和区域的梯度趋向于0，易产生提取消失。</li>
<li>而relu在大于0的部分梯度为常数，所以不会有梯度弥散现象。</li>
<li>Relu的导数计算的更快。</li>
<li>Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。</li>
</ol>
<h2><span id="11-神经网络中权重共享的是">11. 神经网络中权重共享的是？</span></h2><p>卷积神经网络、循环神经网络。解析：通过网络结构直接解释</p>
<h2><span id="12-神经网络激活函数">12. 神经网络激活函数？</span></h2><p>sigmod、tanh、relu</p>
<p>解析：需要掌握函数图像，特点，互相比较，优缺点以及改进方法</p>
<h2><span id="13-在深度学习中通常会finetuning已有的成熟模型再基于新数据修改最后几层神经网络权值为什么">13. 在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？</span></h2><p>实践中的数据集质量参差不齐，可以使用训练好的网络来进行提取特征。把训练好的网络当做特征提取器。</p>
<h2><span id="14-画gru-lstm-rnn结构图">14. 画GRU、LSTM、RNN结构图</span></h2><p>先画图，写公式，再说区别</p>
<p>区别：</p>
<ol>
<li>RNN在处理long term memory的时候存在缺陷，因此LSTM应运而生。</li>
<li>LSTM是一种变种的RNN，它的精髓在于引入了细胞状态这样一个概念，不同于RNN只考虑最近的状态，LSTM的细胞状态会决定哪些状态应该被留下来，哪些状态应该被遗忘。</li>
</ol>
<p>RNN</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/311436_1552882148264_F085D21D1BB5554A2B17E0B4905A207A" alt="img"></p>
<p>LSTM</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/311436_1552882134531_88F162FE8ECCDD9CB2F430E751687AF3" alt="img"></p>
<h3><span id="141-lstm-与-gru-结构的区别">14.1 LSTM 与 GRU 结构的区别</span></h3><ol>
<li>GRU和LSTM的性能在很多任务上不分伯仲。</li>
<li>GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。</li>
<li>从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。</li>
</ol>
<h3><span id="142-rnn容易梯度消失怎么解决">14.2 RNN容易梯度消失，怎么解决？</span></h3><p>1）、梯度裁剪（Clipping Gradient）</p>
<p>既然在BP过程中会产生梯度消失（就是偏导无限接近0，导致长时记忆无法更新），那么最简单粗暴的方法，设定阈值，当梯度小于阈值时，更新的梯度为阈值。</p>
<p>优点：简单粗暴</p>
<p>缺点：很难找到满意的阈值</p>
<p>2）、LSTM（Long Short-Term Memory）</p>
<p>增加三个门和记忆单元进行处理，能够很大程度上保存梯度信息，防止梯度消失，同时模型能够自动学习更新参数。</p>
<h2><span id="15-attention机制的作用">15. Attention机制的作用</span></h2><p>注意力机制的本质就是定位到感兴趣的信息，抑制无用信息，结果通常都是以概率图或者概率特征向量的形式展示。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息，从而提高输出的质量。</p>
<h2><span id="16-lstm和gru的原理">16. Lstm和Gru的原理</span></h2><p>Lstm由输入门、遗忘门、输出门和一个cell组成。</p>
<ol>
<li>第一步是决定从cell状态中丢弃什么信息；</li>
<li>然后在决定有多少新的信息进入到cell状态中；</li>
<li>最终基于目前的cell状态决定输出什么样的信息。</li>
</ol>
<p>GRU由重置门和更新门组成，其输入为前一时刻隐藏层的输出和当前的输入，输出为下一时刻隐藏层的信息。</p>
<ol>
<li>重置门用来计算候选隐藏层的输出，其作用是控制保留多少前一时刻的隐藏层。</li>
<li>更新门的作用是控制加入多少候选隐藏层的输出信息，从而得到当前隐藏层的输出。</li>
</ol>
<h2><span id="17-什么是dropoutdropconnect">17. 什么是dropout，DropConnect</span></h2><p>dropout：在神经网络的训练过程中，对于神经单元按一定的概率将其随机从网络中丢弃，迫使网络在每个训练步骤中适应不同的连接，从而达到对于每个mini-batch都是在训练不同网络的效果，防止过拟合。</p>
<p>DropConnect：</p>
<ol>
<li>是为了提高 Deep Network 的泛化能力的，两者都号称是对 Dropout (Dropout简单理解)的改进。</li>
<li>DropConnect 的思想也很简单，与Dropout不同的是，它不是随机将隐含层节点的输出清0，而是将节点中的每个与其相连的输入权值以1-p的概率清0。（一个是输出，一个是输入）（这里需要注意的是，dropconnect对输入权重进行置0不是真的把权重w变成0，只是在计算上让它等于0而已。。。后续权重还是保持和上一轮一样的。。）</li>
</ol>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/v2-514b53d46fcf34cafd55b27f207884ec_720w.jpg" alt="img"></p>
<h2><span id="18-防止过拟合">18. 防止过拟合</span></h2><ul>
<li>提前终止（当验证集上的效果变差的时候）</li>
<li>L1和L2正则化加权</li>
<li>soft weight sharing，权重共享有利于防止过拟合</li>
<li>dropout</li>
<li>BatchNormalization ；</li>
<li>网络bagging</li>
</ul>
<h2><span id="19-深度学习了解多少有看过底层代码吗">19. 深度学习了解多少，有看过底层代码吗</span></h2><h2><span id="20-adam">20. Adam</span></h2><p>Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的 <strong>一阶矩估计</strong> 和 <strong>二阶矩估计</strong> 而为不同的参数设计独立的自适应性学习率。</p>
<h2><span id="21-attention机制">21. attention机制</span></h2><p>Attention简单理解就是权重分配，以seq2seq中的attention公式作为讲解。就是对输入的每个词分配一个权重，权重的计算方式为与解码端的隐含层时刻作比较，得到的权重的意义就是权重越大，该词越重要。最终加权求和。</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/311436_1552881536175_D404B240266A897C983A190746BE361D" alt="img"></p>
<h2><span id="22-rnn梯度消失问题为什么lstm和gru可以解决此问题">22. RNN梯度消失问题，为什么LSTM和GRU可以解决此问题</span></h2><ol>
<li>RNN由于网络较深，后面层的输出误差很难影响到前面层的计算，RNN的某一单元主要受它附近单元的影响。</li>
<li>而LSTM因为可以通过阀门记忆一些长期的信息，相应的也就保留了更多的梯度。</li>
<li>而GRU也可通过重置和更新两个阀门保留长期的记忆，也相对解决了梯度消失的问题。</li>
</ol>
<h2><span id="23-gan网络的思想">23. GAN网络的思想</span></h2><ol>
<li>GAN用一个生成模型和一个判别模型，判别模型用于判断给定的图片是不是真实的图片，生成模型自己生成一张图片和想要的图片很像；</li>
<li>开始时两个模型都没有训练；</li>
<li>然后两个模型一起进行对抗训练；</li>
<li>生成模型产生图片去欺骗判别模型，判别模型去判别真假，最终两个模型在训练过程中，能力越来越强最终达到稳态。</li>
</ol>
<h2><span id="24-11的卷积作用">24. 1*1的卷积作用</span></h2><p>实现跨通道的交互和信息整合，实现卷积核通道数的降维和升维，可以实现多个feature map的线性组合，而且可是实现与全连接层的等价效果。</p>
<h2><span id="25-怎么提升网络的泛化能力">25. 怎么提升网络的泛化能力</span></h2><ol>
<li>数据上：<ol>
<li>收集更多的数据；</li>
<li>对数据做缩放和变换；</li>
<li>特征组合和重新定义问题。</li>
</ol>
</li>
<li>算法调优上：<ol>
<li>权重的初始化，用小的随机数初始化权重。</li>
<li>对学习率进行调节；</li>
<li>尝试选择合适的激活函数；</li>
<li>调整网络的拓扑结构；</li>
<li>调节batch和epoch的大小；</li>
<li>添加正则化的方法；</li>
<li>尝试使用其它的优化方法,使用early stopping。</li>
</ol>
</li>
</ol>
<h2><span id="26-卷积层和池化层有什么区别">26. 卷积层和池化层有什么区别</span></h2><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>卷积层</th>
<th>池化层</th>
</tr>
</thead>
<tbody>
<tr>
<td>功能</td>
<td>提取特征</td>
<td>压缩特征图，提取主要特征</td>
</tr>
<tr>
<td>操作</td>
<td>可惜是二维的，对于三维数据比如RGB图像（3通道），卷积核的深度必须同输入的通道数，输出的通道数等于卷积核的个数。</td>
<td>池化只是在二维数据上操作的，因此不改变输入的通道数。对于多通道的输入，这一点和卷积区别很大。</td>
</tr>
<tr>
<td>特性</td>
<td>权值共享：减少了参数的数量，并利用了图像目标的位置无关性。</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="27-图像检测算法了解">27. 图像检测算法了解</span></h2><h2><span id="28什么是seq2seq-model">28.什么是seq2seq model</span></h2><p>Seq2seq属于encoder-decoder结构的一种，利用两个RNN，一个作为encoder一个作为decoder。Encoder负责将输入序列压缩成指定长度的向量，这个向量可以看作这段序列的语义，而decoder负责根据语义向量生成指定的序列。</p>
<h2><span id="29-神经网络为啥用交叉熵">29. 神经网络为啥用交叉熵</span></h2><p>通过神经网络解决多分类问题时使用，最常用的一种方式就是在最后一层设置n个输出节点，即分类任务的目标数为n。假设最后的节点数为N，那么对于每一个样例，神经网络可以得到一个N维的数组作为输出结果，数组中每一个维度会对应一个类别。在最理想的情况下，如果一个样本属于k，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即[0,0,1,0,….0,0]，这个数组也就是样本的Label，是神经网络最期望的输出结果，交叉熵就是用来判定实际的输出与期望的输出的接近程度。</p>
<h3><span id="291-计算公式">29.1 计算公式</span></h3><p>？？？</p>
<h2><span id="30-注意力公式">30. 注意力公式</span></h2><p>Soft attention、global attention、动态attention、Hard attention、静态attention、“半软半硬”的attention （local attention）、强制前向attention</p>
<script type="math/tex; mode=display">
Attention(Query, Source)=\sum^{L_x}_{i=1}\alpha_i\cdot Value_i</script><h2><span id="31-lenet-alexnet-vgg-resnet架构区别">31. LeNet、AlexNet、VGG、ResNet架构区别</span></h2><h2><span id="32-dnn的梯度更新方式">32. DNN的梯度更新方式</span></h2><ol>
<li>批量梯度下降法BGD：<ol>
<li>批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新</li>
</ol>
</li>
<li>随机梯度下降法SGD<ol>
<li>由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。</li>
</ol>
</li>
<li>小批量梯度下降法MBGD<ol>
<li>有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。</li>
</ol>
</li>
</ol>
<h2><span id="33-cnn为什么比dnn在图像识别上更好">33. CNN为什么比DNN在图像识别上更好</span></h2><ol>
<li>DNN的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同）；</li>
<li>CNN的输入可以是tensor，例如二维矩阵，通过filter获得局部特征，较好的保留了平面结构信息。</li>
</ol>
<h2><span id="34-使用的-cnn-模型权重之间有关联吗">34. 使用的 CNN 模型权重之间有关联吗？</span></h2><p>权重之间有关联。CNN是权重共享，减少了参数的数量。每个卷积核对图像进行特征提取，就会得到一个Feature Map。</p>
<h2><span id="35-用过哪些-optimizer效果如何">35. 用过哪些 Optimizer，效果如何</span></h2><p>1）SGD；2）Momentum；3）Nesterov；4）Adagrad；5）Adadelta；6）RMSprop；7）Adam；8）Adamax；9）Nadam。</p>
<p>（1）对于稀疏数据，尽量使用学习率可自适应的算法，不用手动调节，而且最好采用默认参数。（2）SGD通常训练时间最长，但是在好的初始化和学习率调度方案下，结果往往更可靠。但SGD容易困在鞍点，这个缺点也不能忽略。（3）如果在意收敛的速度，并且需要训练比较深比较复杂的网络时，推荐使用学习率自适应的优化方法。（4）Adagrad，Adadelta和RMSprop是比较相近的算法，表现都差不多。（5）在能使用带动量的RMSprop或者Adam的地方，使用Nadam往往能取得更好的效果。</p>
<h2><span id="36-图像基础传统图像处理方法知道哪些图像对比度增强说一下">36. 图像基础：传统图像处理方法知道哪些，图像对比度增强说一下</span></h2><p>数字图像处理常用方法：</p>
<ol>
<li>图像变换；</li>
<li>图像编码压缩；</li>
<li>图像增强和复原；</li>
<li>图像分割；</li>
<li>图像描述；</li>
<li>图像分类（识别）</li>
</ol>
<p>全局对比度增强</p>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/acg.gy_42.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/%5Bobject%20Object%5D" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/images/loading.gif" data-original="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/images/loading.gif" data-original="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/05/07/ji-qi-xue-xi-mian-jing/"><img class="prev-cover" src="/images/loading.gif" data-original="/img/acg.gy_42.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习面经</div></div></a></div><div class="next-post pull-right"><a href="/2022/05/07/paper/attention-is-all-you-need/"><img class="next-cover" src="/images/loading.gif" data-original="/img/acg.gy_42.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Attention Is All You Need</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">深度学习面经</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">1. BatchNormalization的作用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 BN层的作用，为什么要在后面加伽马和贝塔，不加可以吗</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">2. 梯度消失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.3.</span> <span class="toc-text">3. 梯度爆炸</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.4.</span> <span class="toc-text">4. 循环神经网络，为什么好?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.5.</span> <span class="toc-text">5. 什么是Group Convolution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.6.</span> <span class="toc-text">6. 什么是RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.7.</span> <span class="toc-text">7. 训练过程中，若一个模型不收敛，那么是否说明这个模型无效？导致模型不收敛的原因有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.8.</span> <span class="toc-text">8. VGG使用3*3卷积核的优势是什么?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.9.</span> <span class="toc-text">9. 图像处理中锐化和平滑的操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.10.</span> <span class="toc-text">10. 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.10.1.</span> <span class="toc-text">10.1 Relu比Sigmoid的效果好在哪里?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.11.</span> <span class="toc-text">11. 神经网络中权重共享的是？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.12.</span> <span class="toc-text">12. 神经网络激活函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.13.</span> <span class="toc-text">13. 在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.14.</span> <span class="toc-text">14. 画GRU、LSTM、RNN结构图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.14.1.</span> <span class="toc-text">14.1 LSTM 与 GRU 结构的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.14.2.</span> <span class="toc-text">14.2 RNN容易梯度消失，怎么解决？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.15.</span> <span class="toc-text">15. Attention机制的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.16.</span> <span class="toc-text">16. Lstm和Gru的原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.17.</span> <span class="toc-text">17. 什么是dropout，DropConnect</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.18.</span> <span class="toc-text">18. 防止过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.19.</span> <span class="toc-text">19. 深度学习了解多少，有看过底层代码吗</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.20.</span> <span class="toc-text">20. Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.21.</span> <span class="toc-text">21. attention机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.22.</span> <span class="toc-text">22. RNN梯度消失问题，为什么LSTM和GRU可以解决此问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.23.</span> <span class="toc-text">23. GAN网络的思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.24.</span> <span class="toc-text">24. 1*1的卷积作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.25.</span> <span class="toc-text">25. 怎么提升网络的泛化能力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.26.</span> <span class="toc-text">26. 卷积层和池化层有什么区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.27.</span> <span class="toc-text">27. 图像检测算法了解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.28.</span> <span class="toc-text">28.什么是seq2seq model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.29.</span> <span class="toc-text">29. 神经网络为啥用交叉熵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.29.1.</span> <span class="toc-text">29.1 计算公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.30.</span> <span class="toc-text">30. 注意力公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.31.</span> <span class="toc-text">31. LeNet、AlexNet、VGG、ResNet架构区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.32.</span> <span class="toc-text">32. DNN的梯度更新方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.33.</span> <span class="toc-text">33. CNN为什么比DNN在图像识别上更好</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.34.</span> <span class="toc-text">34. 使用的 CNN 模型权重之间有关联吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.35.</span> <span class="toc-text">35. 用过哪些 Optimizer，效果如何</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.36.</span> <span class="toc-text">36. 图像基础：传统图像处理方法知道哪些，图像对比度增强说一下</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Xinle Guo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">来自大哥&曼曼吖のBLOG</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '5oH1XuMJszv0RODd3RHom8Gr-gzGzoHsz',
      appKey: 'a63WFYIeCmIrHF0qM1auaJHd',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: 
    }, ))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/js/jquery.min.js"></script><script src="/js/fishes.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="true" data-text="I Love You,大哥,曼曼吖" data-fontsize="15px" data-random="true" async="async"></script><script>(function(d, w, c) {
    w.ChatraID = 'sT7pbp3WZYkxFwd7Q';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-footer-beautify/lib/swiperbdage_init.min.js"></script><!-- hexo injector body_end end -->
        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=i;var e=n.imageLazyLoadSetting.isSPA,r=(n.imageLazyLoadSetting.preloadRatio,Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));function i(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight+240||document.documentElement.clientHeight+240)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},t.src!==i&&(n.src=i)}()}i(),n.addEventListener("scroll",function(){var t,e;t=i,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"live2d-widget-model-epsilon2_1"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>