<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>激活函数+数据稳定性 | 曼曼吖の博客</title><meta name="keywords" content="机器学习,数据稳定性,激活函数"><meta name="author" content="Xinle Guo,xinleguo@outlook.com"><meta name="copyright" content="Xinle Guo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="基于数据稳定性对深度学习进行理解，并对激活函数进行解释">
<meta property="og:type" content="article">
<meta property="og:title" content="激活函数+数据稳定性">
<meta property="og:url" content="https://xiaomanzhan.club/2022/04/30/machine-learning/shu-ju-wen-ding-xing-and-ji-huo-han-shu/index.html">
<meta property="og:site_name" content="曼曼吖の博客">
<meta property="og:description" content="基于数据稳定性对深度学习进行理解，并对激活函数进行解释">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg">
<meta property="article:published_time" content="2022-04-30T08:26:20.000Z">
<meta property="article:modified_time" content="2022-05-07T13:43:22.412Z">
<meta property="article:author" content="Xinle Guo">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="数据稳定性">
<meta property="article:tag" content="激活函数">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xiaomanzhan.club/2022/04/30/machine-learning/shu-ju-wen-ding-xing-and-ji-huo-han-shu/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '激活函数+数据稳定性',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-07 21:43:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="/css/runtime.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="曼曼吖の博客" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/loading.gif" data-original="/img/avatar1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">101</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">76</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-image"></i><span> 时光长廊</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">曼曼吖の博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-image"></i><span> 时光长廊</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">激活函数+数据稳定性</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-30T08:26:20.000Z" title="发表于 2022-04-30 16:26:20">2022-04-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-07T13:43:22.412Z" title="更新于 2022-05-07 21:43:22">2022-05-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="激活函数+数据稳定性"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1><span id="激活函数数据稳定性">激活函数+数据稳定性</span></h1><h2><span id="1-数值的稳定性">1. 数值的稳定性</span></h2><blockquote>
<p>在说激活函数之前，先介绍一下数据的稳定性</p>
</blockquote>
<p>当深度网络很深的时候，数据的数值信息是非常不稳定的，不利于模型参数的优化。</p>
<h3><span id="11-神经网络的梯度">1.1 神经网络的梯度</span></h3><p>一个 <code>d</code> 层的神经网络，$h^t=f_t(h^{t-1})$，$y=\ell\circ f_d\circ\dots\circ f_1(x)$</p>
<p>计算损失 $\ell$ 关于参数 $W_t$ 的梯度信息：</p>
<script type="math/tex; mode=display">
\frac{\delta\ell}{\delta W^t}=\frac{\delta\ell}{\delta h^d}\frac{\delta h^d}{\delta h^{d-1}}\dots\frac{\delta h^{t+1}}{\delta h^{t}}\frac{\delta h^t}{\delta W^t}</script><h3><span id="12-数值稳定性的常见两个问题">1.2 数值稳定性的常见两个问题</span></h3><ol>
<li><p>梯度爆炸：</p>
<p>$1.5^{100}\approx4\times 10^{17}$</p>
</li>
<li><p>梯度消失：</p>
<p>$0.8^{100}\approx4\times 10^{17}$</p>
</li>
</ol>
<p>举例说明（MLP），计算第t层的导数：</p>
<ol>
<li>$ h^t=f_t(h^{t-1})=\sigma(W^th^{t-1})$，$\sigma$ 是激活函数；</li>
<li>$\frac{\delta h^{t}}{\delta h^{t-1}}=diag(\sigma’(W^th^{t-1}))(W^t)^T$ ，$\sigma’$ 是$\sigma$ 的导数函数，求导目的是计算梯度更新权重</li>
<li>$\prod_{i = t}^{d-1}\frac{\delta h^{i+1}}{\delta h^{i}}=\prod_{i=t}^{d-1}diag(\sigma’(W^ih^{i-1}))(W^i)^T$</li>
</ol>
<h4><span id="121-梯度爆炸">1.2.1 梯度爆炸</span></h4><p>简单来说就是梯度误差的积累（梯度误差总是$&gt;1$的）引起的梯度爆炸，假设我们此处使用 ReLU 作为激活函数 </p>
<script type="math/tex; mode=display">
\sigma(x)=max(0,x)~~and~~\sigma'=
\begin{cases}
1,\quad if\quad x> 0 \\[2ex]
0,\quad otherwise
\end{cases}
\tag{1}</script><p>使用上面激活函数在计算 $\prod_{i = t}^{d-1}\frac{\delta h^{i+1}}{\delta h^{i}}=\prod_{i=t}^{d-1}diag(\sigma’(W^ih^{i-1}))(W^i)^T$ 的一些元素会来自于 $\prod_{i = t}^{d-1}(W^i)^T$。如果 d-t 很大（即网络层数很深），假设W中存在很多大于1的数，那么最终结果值将会很大。</p>
<p><strong>导致的问题：</strong></p>
<ol>
<li><p>值超出值域 (infinity)</p>
<ol>
<li>对于 16 位浮点数尤为严重 (数值区间 6e-5 - 6e4)</li>
</ol>
</li>
<li><p>对学习率敏感</p>
<ol>
<li><p>如果学习率太大 -&gt; 大参数值 -&gt; 更大的梯度;</p>
<p>（解释：当学习率大了，取值大的参数会迅速变大，一个参数过大会导致同一层的别的参数也变大，进而是梯度更大）</p>
</li>
<li><p>如果学习率太小 -&gt; 训练无进展；</p>
<p>（解释：学习率太小的话，对参数$W$ 的更新就很小）</p>
</li>
<li><p>我们可能需要在训练过程中不断调整学习率</p>
</li>
</ol>
</li>
</ol>
<p>解决方式：</p>
<ol>
<li>重现设计神经网络：减少网络层数、减小batch szie、截断。</li>
<li>使用LSTM；</li>
<li>使用梯度裁剪：<code>clipnorm=1.0 clipvalue=0.5</code>；</li>
<li>使用权重正则：L1 &amp; L2</li>
</ol>
<h4><span id="122-梯度消失">1.2.2 梯度消失</span></h4><p>使用sigmoid 作为激活函数</p>
<script type="math/tex; mode=display">
\sigma=\frac{1}{1+e^{-x}}\\
\sigma'(x)=\sigma(x)(1-\sigma(x))</script><p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/image-20220430171424982.png" alt="image-20220430171424982" style="zoom:67%;"></p>
<p>使用上面激活函数在计算 $\prod_{i = t}^{d-1}\frac{\delta h^{i+1}}{\delta h^{i}}=\prod_{i=t}^{d-1}diag(\sigma’(W^ih^{i-1}))(W^i)^T$ 的元素值是 d-t 个小数值的乘积：$0.8^{100}\approx4\times 10^{17}$</p>
<p><strong>导致的问题</strong></p>
<ol>
<li>梯度值变为0<ol>
<li>对16位浮点数尤为严重</li>
</ol>
</li>
<li>训练没有进展<ol>
<li>不管如何选择学习率</li>
</ol>
</li>
<li>对底部层尤为严重<ol>
<li>仅仅顶层训练的较好；</li>
<li>无法让神经网络更深</li>
</ol>
</li>
</ol>
<h2><span id="2-让训练更加稳定">2. 让训练更加稳定</span></h2><p>想要让训练更加稳定就必须将梯度控制在合理的范围（我们的目标），解决方案如下：</p>
<ol>
<li>将乘法变加法：如ResNet、LSTM神经网络；</li>
<li>归一化：梯度归一化、梯度裁剪；</li>
<li>合理的权重初始化和激活函数</li>
</ol>
<h3><span id="21-让每层的方差是一个常数">2.1 让每层的方差是一个常数</span></h3><p>数据归一化：让每层的方差是一个常数。将每层的输出和梯度度看做随机变量，并且让他们的均值和方差保持一致。</p>
<h3><span id="22-权重初始化">2.2 权重初始化</span></h3><ul>
<li>在合理的区间里随机初始参数。</li>
<li>因为在训练开始的时候更容易有数值不稳定的情况<ul>
<li>在原理最优解的地方损失函数表面可能很复杂</li>
<li>最优解附近表面会比较评</li>
</ul>
</li>
<li>使用 $\N(0,0.01)$ 来初始化可能对小网络没问题，但是不能保证深度神经网络</li>
</ul>
<h3><span id="23-检测常用的激活函数">2.3 检测常用的激活函数</span></h3><p>使用泰勒展开：</p>
<script type="math/tex; mode=display">
sigmoid(x)=\frac{1}{2}+\frac{x}{4}-\frac{x^3}{48}+O(x^5)</script><script type="math/tex; mode=display">
\tanh(x)=0+x-\frac{x^3}{3}+O(x^5)</script><script type="math/tex; mode=display">
relu(x)=0+x\quad for~x\geq0</script><p>调整之后的sigmoid函数（scaled sigmoid函数），变换之后能够解决之前带来的梯度消失的问题</p>
<script type="math/tex; mode=display">
4\times sigmoid(x)-2</script><p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/image-20220430181715053.png" alt="image-20220430181715053"></p>
<h2><span id="3-激活函数">3. 激活函数</span></h2><p>激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。类似于人类大脑中基于神经元的模型，激活函数最终决定了要发射给下一个神经元的内容。</p>
<ul>
<li>Softmax</li>
<li>Sigmoid</li>
<li>Tanh</li>
<li><strong>ReLU</strong>（以及基于ReLU的改进系列：Leaky ReLU、ELU、PReLU等）</li>
<li>Gelu（Gaussian Error Linear Unit，2016年被提出，直到2018年Bert开始使用才被重视）</li>
<li>Swish（2017年google提出）</li>
</ul>
<h3><span id="31-sigmoid">3.1 Sigmoid</span></h3><script type="math/tex; mode=display">
f(x)=\frac{1}{1+e^{-x}}</script><p>如下图所示：</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/nhi9swy8ln.png" alt="img" style="zoom:67%;"></p>
<p>在什么情况下适合使用 Sigmoid 激活函数呢？</p>
<ul>
<li>Sigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到 1，因此它对每个神经元的输出进行了归一化；</li>
<li>用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；</li>
<li>梯度平滑，避免「跳跃」的输出值；</li>
<li>函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；</li>
<li>明确的预测，即非常接近 1 或 0。</li>
</ul>
<h4><span id="311-sigmoid的变体">3.1.1 Sigmoid的变体</span></h4><p>对于 Sigmoid 可能存在的梯度消失的问题，可以考虑将Sigmoid输出的范围扩大到 <code>[-1, 1]</code> 之间，能够在一定程度上缓解梯度消失。即 Scale Sigmoid</p>
<script type="math/tex; mode=display">
f(x)=\frac{4}{1+e^{-x}}-2</script><h3><span id="32-tanh">3.2 Tanh</span></h3><p>Tanh激活函数又名 双曲正切激活函数，公式如下所示：</p>
<script type="math/tex; mode=display">
f(x)=tanh(x)=\frac{2}{1+e^{-2x}}-1</script><p>如下图所示：</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/gm9p8du88z.png" alt="img"></p>
<p>与 sigmoid 的优缺点：</p>
<ol>
<li>当输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新。</li>
<li>二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；</li>
<li>在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。</li>
</ol>
<p><strong>注意：</strong>在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。</p>
<h3><span id="33-relu">3.3 ReLU</span></h3><script type="math/tex; mode=display">
\sigma(x)=max(0,x)</script><p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/ck888so4in.png" alt="img"></p>
<p>ReLU 函数是深度学习中较为流行的一种激活函数，相比于 sigmoid 函数和 tanh 函数，它具有如下优点：</p>
<ul>
<li>当输入为正时，不存在梯度饱和问题。</li>
<li>计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。</li>
</ul>
<p>当然，它也有缺点：</p>
<ol>
<li>Dead ReLU 问题（<strong>神经元“死亡”问题</strong>）。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；</li>
<li>我们发现 ReLU 函数的输出为 0 或正数，这意味着 ReLU 函数不是以 0 为中心的函数。</li>
</ol>
<h3><span id="34-leaky-relu">3.4 Leaky ReLU</span></h3><p>它是一种专门设计用于解决 Dead ReLU 问题的激活函数：</p>
<script type="math/tex; mode=display">
f(y_i)=
\begin{cases}
y_i,\quad if~y_i> 0 \\[2ex]
a_iy_i,\quad if~y_i\leq 0
\end{cases}
\tag{1}</script><p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/zdls8bt48h.png" alt="img"></p>
<p>Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题；</p>
<ol>
<li>leak 有助于扩大 ReLU 函数的范围，通常 a 的值为 0.01 左右；</li>
<li>Leaky ReLU 的函数范围是（负无穷到正无穷）。</li>
</ol>
<p>注意：从理论上讲，Leaky ReLU 具有 ReLU 的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明 Leaky ReLU 总是比 ReLU 更好。</p>
<h3><span id="35-elu">3.5 ELU</span></h3><script type="math/tex; mode=display">
g(x)=ELU(x)=
\begin{cases}
x,\quad\quad\quad\quad if~y_i> 0 \\[2ex]
\alpha(e^x-1),\quad if~y_i\leq 0
\end{cases}
\tag{1}</script><p>ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/ygenzy9ncs.png" alt="img"></p>
<p>显然，ELU 具有 ReLU 的所有优点，并且：</p>
<ul>
<li>没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；</li>
<li>ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；</li>
<li>ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。</li>
</ul>
<p>一个小问题是它的计算强度更高。与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。</p>
<h3><span id="36-gelu">3.6 GELU</span></h3><p>高斯误差线性单元激活函数，在最近的Transformer模型（谷歌的BERT和OpenAI的GPT-2）中得到了应用,GELU的论文来自2016年，但是最近才引起关注，这种激活函数的形式为：</p>
<script type="math/tex; mode=display">
GELU(x)=xP(X\leq{x})=x\Phi(x)\\</script><p>其中 $\Phi(x)$ 指的是 $x$ 的高斯正态分布的累积分布，完整形式如下：</p>
<script type="math/tex; mode=display">
xP(X\leq{x})=x\int^x_{-\infty}\frac{e^{-\frac{(X-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma}}dX\\
=0.5x(1+tanh[\sqrt{\frac{2}{\pi}}(x+0.044715x^3)])=x\sigma(1.702x)</script><p>GELU的函数图像（左）及其导数图像（右）如下图所示：</p>
<p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/gelu_viz-1.png" alt="GELU"></p>
<p>可以看出，当x越大的时候，就越有可能被保留，x越小就越有可能被归置为0.</p>
<h3><span id="37-swish">3.7 Swish</span></h3><script type="math/tex; mode=display">
f(x)=x\cdot sigmoid(\beta x)</script><p><img src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/content/20200703101353980.png" alt="在这里插入图片描述" style="zoom:67%;"></p>
<ul>
<li><p>当 $\beta=0$ 时，Swish激活函数变为线性函数 $f(x) = x/2$ </p>
</li>
<li><p>当 $ \beta= \infty$ 时，Swish激活函数变为0或x，相当于 Relu，</p>
</li>
</ul>
<p>所以，Swish函数可以看作是介于线性函数与ReLU函数之间的平滑函数。</p>
<h2><span id="4-总结">4. 总结</span></h2><ol>
<li>权重的初始值和激活函数的选取可以提升数值稳定性；</li>
<li>Sigmoid和Tanh很相似，但后者的梯度更陡，都会存在梯度消失、梯度爆炸的风险。</li>
<li>ReLU函数一大优点是不会激活所有神经元，前向传播时如果输入是负值，则该神经元不会被激活，那么反向传播时梯度就是0，该神经元权重不会更新，就会变成死神经元。</li>
<li>基于ReLU死神经元的这种缺点，才提出了Leaky ReLU、ELU、PReLU等。</li>
<li>Gelu和Swish很相似，在论文中的对比结果也很相近，他们的区别在于前者固定了系数为1.702，后者的系数是可以选择固定为常数，也可以选择为可训练的参数。</li>
</ol>
<h2><span id="5-相关面试题">5. 相关面试题</span></h2><p><strong>1.什么是激活函数，为什么需要激活函数？</strong></p>
<p>激活函数是在神经网络层间输入与输出之间的一种函数变换，目的是为了加入非线性因素，增强模型的表达能力。</p>
<p>如果没有激活函数，那么模型就只有线性变换，可想而知线性模型能表达的空间是有限的。而激活函数引入了非线性因素，比线性模型拥有更大的模型空间。</p>
<p><strong>2.了解那些激活函数以及应用？</strong></p>
<p>回答主要分两类（饱和/非饱和），以及应用场景等。有时候可能特定到具体经典模型，比如LSTM用到Tanh，Transfromer中用到的ReLU，Bert中的GeLU，YOLO的Leaky ReLU等。</p>
<p><strong>3.梯度消失与梯度爆炸现象与原因以及解决办法？</strong></p>
<p>参看梯度消失与梯度爆炸部分。</p>
<p><strong>4.ReLU激活函数为什么会出现死神经元，解决办法？</strong></p>
<ol>
<li>除上文提到输入为负值时，ReLU的梯度为0造成神经元死亡。</li>
<li>还有Learning rate太高导致在训练过程中参数更新太大 。</li>
<li>初始化参数的问题。 —&gt; 采用Xavier初始化方法。</li>
</ol>
<p>解决办法主要有：</p>
<ol>
<li>优化参数。 </li>
<li>避免将learning rate设置太大，或者使用Adam等自动调节learning rate的方法。</li>
<li>更换激活函数。</li>
</ol>
<p><strong>5.如何选择激活函数？</strong></p>
<ol>
<li>除非在二分类问题中，否则请小心使用Sigmoid函数。</li>
<li>对于长序列的问题，隐藏层中尽量避免使用Sigmoid和Tanh，会造成梯度消失的问题；</li>
<li>可以试试Tanh，不过大多数情况下它的效果会比不上 ReLU 和 Maxout。</li>
<li>如果你不知道应该使用哪个激活函数， 那么请优先选择ReLU。</li>
<li>如果你使用了ReLU， 需要注意一下Dead ReLU问题， 此时你需要仔细选择 Learning rate， 避免出现大的梯度从而导致过多的神经元 “Dead” 。</li>
<li>如果发生了Dead ReLU问题， 可以尝试一下leaky ReLU，ELU等ReLU变体， 说不定会有很好效果。</li>
<li>Relu在Gelu出现之前在大多数情况下比较通用，但也只能在隐层中使用；</li>
<li>现在2021年了，隐藏层中主要的选择肯定优先是Gelu、Swish了。</li>
</ol>
<p><strong>6. transformer FFN层用的激活函数是什么？为什么？</strong></p>
<p>ReLU.</p>
<blockquote>
<p>可以把ReLU的优点提一下，然后提一下解决ReLU死神经元问题的方案。</p>
</blockquote>
<p>ReLU的优点是收敛速度快、不会出现梯度消失or爆炸的问题、计算复杂度低。</p>
<p>出现死神经元的原因及解决方案：</p>
<ul>
<li>初始化参数的问题。 —&gt; 采用Xavier初始化方法。</li>
<li>learning rate太高导致在训练过程中参数更新太大 。==&gt;避免将learning rate设置太大，或者使用Adam等自动调节learning rate的方法。</li>
<li>更换激活函数。 —&gt; Leaky ReLU、PReLU、ELU等都是为了解决死神经元的问题。</li>
</ul>
<p><strong>7. Bert、GPT、GPT2中用的激活函数是什么？为什么？</strong></p>
<p>Gelu.</p>
<blockquote>
<p>使用Gelu时，一般优化器都会选择动态更新lr的方法。</p>
</blockquote>
<p>Bert、GPT、GPT2、RoBERTa、ALBERT都是用的Gelu。</p>
<p>直观理解：x做为神经元的输入，P(X&lt;=x)越大，x就越有可能被保留；否则越小，激活函数输出就趋近于0.</p>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E7%A8%B3%E5%AE%9A%E6%80%A7/">数据稳定性</a><a class="post-meta__tags" href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">激活函数</a></div><div class="post_share"><div class="social-share" data-image="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/%5Bobject%20Object%5D" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/images/loading.gif" data-original="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/images/loading.gif" data-original="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/04/30/machine-learning/softmax-hui-gui-and-sun-shi-han-shu/"><img class="prev-cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Softmax回归+损失函数</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/30/machine-learning/shu-ju-yu-chu-li-shu-ju-gui-yi-hua/"><img class="next-cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据预处理-数据归一化</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/03/30/machine-learning/apriori-suan-fa/" title="Apriori算法"><img class="cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-30</div><div class="title">Apriori算法</div></div></a></div><div><a href="/2022/04/30/machine-learning/softmax-hui-gui-and-sun-shi-han-shu/" title="Softmax回归+损失函数"><img class="cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-30</div><div class="title">Softmax回归+损失函数</div></div></a></div><div><a href="/2022/05/01/machine-learning/tensorboardx/" title="tensorboardX"><img class="cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-01</div><div class="title">tensorboardX</div></div></a></div><div><a href="/2022/04/23/machine-learning/shi-zhan-kaggle-bi-sai-yu-ce-fang-jie/" title="实战Kaggle比赛：预测房价"><img class="cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-23</div><div class="title">实战Kaggle比赛：预测房价</div></div></a></div><div><a href="/2022/04/30/machine-learning/shu-ju-yu-chu-li-shu-ju-gui-yi-hua/" title="数据预处理-数据归一化"><img class="cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-30</div><div class="title">数据预处理-数据归一化</div></div></a></div><div><a href="/2022/03/30/machine-learning/guan-lian-gui-ze-suan-fa/" title="关联规则算法"><img class="cover" src="/images/loading.gif" data-original="http://xiaomanzhan.com.cn/top_img/Machine_Learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-30</div><div class="title">关联规则算法</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">激活函数+数据稳定性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">1. 数值的稳定性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 神经网络的梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 数值稳定性的常见两个问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">1.2.1 梯度爆炸</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">1.2.2 梯度消失</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">2. 让训练更加稳定</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 让每层的方差是一个常数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 权重初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 检测常用的激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.3.</span> <span class="toc-text">3. 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 Sigmoid</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">3.1.1 Sigmoid的变体</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 Tanh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 Leaky ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 ELU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.6.</span> <span class="toc-text">3.6 GELU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.3.7.</span> <span class="toc-text">3.7 Swish</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.4.</span> <span class="toc-text">4. 总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.5.</span> <span class="toc-text">5. 相关面试题</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Xinle Guo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">来自大哥&曼曼吖のBLOG</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '5oH1XuMJszv0RODd3RHom8Gr-gzGzoHsz',
      appKey: 'a63WFYIeCmIrHF0qM1auaJHd',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: 
    }, ))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/js/jquery.min.js"></script><script src="/js/fishes.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="true" data-text="I Love You,大哥,曼曼吖" data-fontsize="15px" data-random="true" async="async"></script><script>(function(d, w, c) {
    w.ChatraID = 'sT7pbp3WZYkxFwd7Q';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-footer-beautify/lib/swiperbdage_init.min.js"></script><!-- hexo injector body_end end -->
        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script><script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=i;var e=n.imageLazyLoadSetting.isSPA,r=(n.imageLazyLoadSetting.preloadRatio,Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));function i(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight+240||document.documentElement.clientHeight+240)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},t.src!==i&&(n.src=i)}()}i(),n.addEventListener("scroll",function(){var t,e;t=i,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"live2d-widget-model-epsilon2_1"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>